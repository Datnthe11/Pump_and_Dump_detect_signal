import requests
import pandas as pd
from datetime import datetime, timedelta
import time
import os
import json

# ======= C·∫•u h√¨nh =======
API_KEY = " "  # Thay b·∫±ng API key th·∫≠t
BASE_URL = "https://api.twitterapi.io/twitter/tweet/advanced_search"

HEADERS = {"X-API-Key": API_KEY}

# Keywords v·ªÅ crypto - Lo·∫°i b·ªè t·ª´ nhi·ªÖu cao
KEYWORDS = (
    # C√°c coin ch√≠nh (B·∫ÆT BU·ªòC ph·∫£i c√≥ √≠t nh·∫•t 1 trong s·ªë n√†y)
    '('
    '$BTC OR $ETH OR $SOL OR $BNB OR $ADA OR $DOGE OR $XRP '
    'OR #Bitcoin OR #Ethereum OR #Solana OR #Crypto OR #Cryptocurrency '
    'OR bitcoin OR ethereum OR solana OR "crypto" '
    
    # Thu·∫≠t ng·ªØ v·ªÅ scam/manipulation (CH·ªà d√πng c·ª•m t·ª´, KH√îNG d√πng t·ª´ ƒë∆°n)
    'OR rugpull OR "rug pull" OR "pump and dump" OR "exit scam" '
    'OR honeypot OR "crypto scam" OR "market manipulation" '
    
    # T√≠n hi·ªáu giao d·ªãch (CH·ªà d√πng c·ª•m t·ª´)
    'OR "buy signal" OR "sell signal" OR "buy now" OR "sell now" '
    'OR "massive buy" OR "massive sell" OR "whale alert" OR "whale movement" '
    'OR "buying opportunity" OR "selling pressure" '
    
    # Thu·∫≠t ng·ªØ crypto ph·ªï bi·∫øn (CH·ªà d√πng c·ª•m t·ª´ ho·∫∑c t·ª´ vi·∫øt hoa ƒë·∫∑c bi·ªát)
    'OR "to the moon" OR "moon soon" OR rekt OR REKT '
    'OR "bull run" OR "bear trap" OR "bull trap" OR "bear market" OR "bull market" '
    'OR FOMO OR HODL OR "buying the dip" OR "diamond hands" OR "paper hands" '
    'OR "100x" OR "10x gem" OR "moon shot" OR "gem alert" '
    'OR "altcoin season" OR "alt season" OR "meme season" '
    'OR shitcoin OR memecoin OR "pump incoming" OR "shilling" '
    'OR "crypto twitter" OR "crypto news" OR "degen" OR "ape in" '
    ')'
)

QUERY_TYPE = "Latest"
TWEETS_PER_DAY = 100  # M·ª•c ti√™u tweets m·ªói ng√†y
OUTPUT_CSV = "tweets_2025Q3_crypto.csv"
OUTPUT_JSON = "tweets_2025Q3_crypto.json"

# Debug mode - B·∫≠t ƒë·ªÉ xem tweets API tr·∫£ v·ªÅ
DEBUG_MODE = True  # ƒê·∫∑t False sau khi test xong

# ‚ö†Ô∏è Th·ªùi gian crawl - Q3/2025 (1/7 - 30/9/2025)
START_DATE = datetime(2025, 7, 1)
END_DATE = datetime(2025, 9, 30)

# ======= H√†m fetch page =======
def fetch_page(query_string, cursor=None):
    """
    Fetch m·ªôt trang tweets t·ª´ API
    API t·ª± ƒë·ªông tr·∫£ max 20 tweets/page
    """
    params = {
        "query": query_string,  # since/until ph·∫£i n·∫±m TRONG query string
        "queryType": QUERY_TYPE
    }
    
    if cursor:
        params["cursor"] = cursor

    try:
        resp = requests.get(BASE_URL, headers=HEADERS, params=params, timeout=30)
        
        if resp.status_code == 200:
            return resp.json()
        elif resp.status_code == 429:
            print(f"‚ö†Ô∏è Rate limit! Ch·ªù 60s...")
            time.sleep(60)
            return fetch_page(query_string, cursor)  # Retry
        elif resp.status_code == 401:
            print(f"‚ùå API key kh√¥ng h·ª£p l·ªá!")
            return None
        else:
            print(f"‚ùå API l·ªói: {resp.status_code} - {resp.text}")
            return None
            
    except requests.RequestException as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi: {e}")
        return None

# ======= Crawl tweet cho 1 ng√†y =======
def fetch_tweets_for_day(day):
    """
    L·∫•y tweets cho m·ªôt ng√†y c·ª• th·ªÉ
    S·ª≠ d·ª•ng since/until TRONG query string theo format Twitter API
    """
    all_tweets = []
    seen_ids = set()
    cursor = None
    page = 0
    
    # Format ng√†y: YYYY-MM-DD (format ng·∫Øn g·ªçn - ƒë√£ test work!)
    date_str = day.strftime("%Y-%m-%d")
    next_day_str = (day + timedelta(days=1)).strftime("%Y-%m-%d")
    
    # ‚ö†Ô∏è G·ªåN H∆†N: Ch·ªâ d√πng coin symbols + since/until
    # Query ph·ª©c t·∫°p l√†m API b·ªè qua filter th·ªùi gian!
    SIMPLE_CRYPTO = '($BTC OR $ETH OR $SOL OR #Bitcoin OR #Ethereum OR #Crypto)'
    
    # ‚úÖ Query ƒë∆°n gi·∫£n: coin + time + language
    query_string = (
        f'{SIMPLE_CRYPTO} '
        f'lang:en -is:retweet -is:quote '
        f'since:{date_str} until:{next_day_str}'
    )
    
    print(f"\nüìÖ ƒêang crawl ng√†y {date_str}...")
    print(f"üìù Query: {query_string}")  # In TO√ÄN B·ªò query ƒë·ªÉ debug
    print(f"üìè Query length: {len(query_string)} chars")
    
    while len(all_tweets) < TWEETS_PER_DAY:
        page += 1
        print(f"  üìÑ Page {page}...", end=" ")
        
        data = fetch_page(query_string, cursor)
        
        if data is None:
            print("‚ùå L·ªói API")
            break
        
        tweets = data.get("tweets", [])
        has_next = data.get("has_next_page", False)
        next_cursor = data.get("next_cursor")
        
        if not tweets:
            print(f"‚úì Kh√¥ng c√≤n tweets (total: {len(all_tweets)})")
            break
        
        # DEBUG: In ra th·ªùi gian c·ªßa tweets
        if DEBUG_MODE and page == 1:
            print(f"\n  üîç DEBUG - API tr·∫£ v·ªÅ {len(tweets)} tweets:")
            for i, t in enumerate(tweets[:3], 1):
                created = t.get("createdAt", "N/A")
                text_preview = t.get("text", "")[:50]
                print(f"     {i}. {created}")
                print(f"        \"{text_preview}...\"")
            print(f"  ", end="")
        
        # L·ªçc v√† th√™m tweets
        new_count = 0
        skipped_count = 0
        for t in tweets:
            tid = t.get("id")
            if tid and tid not in seen_ids:
                # Ki·ªÉm tra th·ªùi gian (nh∆∞ng KH√îNG skip n·∫øu API ƒë√£ filter)
                tweet_date_ok = True
                try:
                    created_at_str = t.get("createdAt", "")
                    if created_at_str:
                        created_at = datetime.strptime(created_at_str, "%a %b %d %H:%M:%S %z %Y")
                        created_at_utc = created_at.astimezone(None).replace(tzinfo=None)
                        
                        # So s√°nh ng√†y (cho ph√©p sai l·ªách 1 ng√†y do timezone)
                        date_diff = abs((created_at_utc.date() - day.date()).days)
                        if date_diff > 1:
                            tweet_date_ok = False
                            skipped_count += 1
                except Exception as e:
                    # N·∫øu parse l·ªói, v·∫´n l·∫•y (tin t∆∞·ªüng API filter)
                    pass
                
                if tweet_date_ok:
                    seen_ids.add(tid)
                    all_tweets.append(t)
                    new_count += 1
                    
                    if len(all_tweets) >= TWEETS_PER_DAY:
                        break
        
        if skipped_count > 0:
            print(f"(skipped {skipped_count} wrong date)", end=" ")
        
        print(f"‚úì +{new_count} tweets (total: {len(all_tweets)})")
        
        # Ki·ªÉm tra c√≥ trang ti·∫øp theo kh√¥ng
        if not has_next or not next_cursor or len(all_tweets) >= TWEETS_PER_DAY:
            break
            
        cursor = next_cursor
        time.sleep(0.5)  # Tr√°nh rate limit
    
    print(f"‚úÖ Ng√†y {date_str}: L·∫•y ƒë∆∞·ª£c {len(all_tweets)} tweets")
    return all_tweets

# ======= L∆∞u CSV =======
def save_csv(tweets, file_path):
    """L∆∞u tweets v√†o CSV (append mode)"""
    if not tweets:
        return
    
    df = pd.json_normalize(tweets)
    
    # Ki·ªÉm tra file ƒë√£ t·ªìn t·∫°i ch∆∞a
    file_exists = os.path.exists(file_path)
    
    # Append n·∫øu file ƒë√£ c√≥, create m·ªõi n·∫øu ch∆∞a
    df.to_csv(
        file_path,
        index=False,
        mode='a' if file_exists else 'w',
        header=not file_exists,
        encoding='utf-8'
    )
    
    print(f"üíæ ƒê√£ l∆∞u {len(tweets)} tweets v√†o {file_path}")

# ======= L∆∞u JSON =======
def save_json(tweets, file_path):
    """L∆∞u tweets v√†o JSON (merge v·ªõi data c≈©)"""
    if not tweets:
        return
    
    existing = []
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                existing = json.load(f)
        except json.JSONDecodeError:
            print(f"‚ö†Ô∏è File JSON b·ªã l·ªói, s·∫Ω t·∫°o m·ªõi")
    
    all_data = existing + tweets
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ ƒê√£ l∆∞u t·ªïng {len(all_data)} tweets v√†o {file_path}")

# ======= H√†m ki·ªÉm tra API key =======
def check_api_key():
    """Ki·ªÉm tra API key c√≥ h·ª£p l·ªá kh√¥ng"""
    if not API_KEY or API_KEY == "":
        print("‚ùå C·∫¢NH B√ÅO: API_KEY tr·ªëng! Vui l√≤ng th√™m API key.")
        return False
    
    # Test API key v·ªõi query ƒë∆°n gi·∫£n
    test_query = "$BTC lang:en"
    data = fetch_page(test_query)
    
    if data is None:
        print("‚ùå API key kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng c√≥ quy·ªÅn truy c·∫≠p")
        return False
    
    print("‚úÖ API key h·ª£p l·ªá!")
    return True

# ======= Main =======
def main():
    print("=" * 70)
    print("üöÄ TWITTER CRYPTO CRAWLER - Q3/2025 (1/7 - 30/9/2025)")
    print("=" * 70)
    
    # Ki·ªÉm tra API key
    if not check_api_key():
        return
    
    # Th√¥ng b√°o th·ªùi gian crawl
    print(f"\nüìÜ Th·ªùi gian crawl:")
    print(f"   T·ª´: {START_DATE.strftime('%d/%m/%Y')}")
    print(f"   ƒê·∫øn: {END_DATE.strftime('%d/%m/%Y')}")
    print(f"   T·ªïng: {(END_DATE - START_DATE).days + 1} ng√†y")
    print(f"   M·ª•c ti√™u: ~{TWEETS_PER_DAY} tweets/ng√†y")
    
    # X√°c nh·∫≠n
    print(f"\n‚ö†Ô∏è  L∆∞u √Ω: D·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c l∆∞u v√†o:")
    print(f"   - {OUTPUT_CSV}")
    print(f"   - {OUTPUT_JSON}")
    
    input("\nüëâ Nh·∫•n Enter ƒë·ªÉ b·∫Øt ƒë·∫ßu crawl...")
    
    # B·∫Øt ƒë·∫ßu crawl
    current_day = START_DATE
    total_tweets = 0
    success_days = 0
    
    while current_day <= END_DATE:
        tweets = fetch_tweets_for_day(current_day)
        
        if tweets:
            save_csv(tweets, OUTPUT_CSV)
            save_json(tweets, OUTPUT_JSON)
            total_tweets += len(tweets)
            success_days += 1
        
        current_day += timedelta(days=1)
        
        # Ngh·ªâ gi·ªØa c√°c ng√†y
        if current_day <= END_DATE:
            print(f"‚è∏Ô∏è  Ch·ªù 2s tr∆∞·ªõc khi crawl ng√†y ti·∫øp theo...")
            time.sleep(2)
    
    print("\n" + "=" * 70)
    print(f"üéâ HO√ÄN TH√ÄNH!")
    print("=" * 70)
    print(f"‚úÖ Crawl th√†nh c√¥ng: {success_days}/{(END_DATE - START_DATE).days + 1} ng√†y")
    print(f"üìä T·ªïng s·ªë tweets: {total_tweets}")
    print(f"üìà Trung b√¨nh: {total_tweets // success_days if success_days > 0 else 0} tweets/ng√†y")
    print(f"üìÅ File CSV: {OUTPUT_CSV}")
    print(f"üìÅ File JSON: {OUTPUT_JSON}")
    print("=" * 70)

if __name__ == "__main__":
    main()